{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Seattle AirBnB dataset analysis\n\nSeattle is a wonderful to visit, and AirBnB is a wonderful way to find a place to stay while visiting any place. Let's see what kind of insight I can gather for my next trip to Seattle from AirBnB's Seattle data.\n\n\n## Procedures\n\n* Explore the dataset\n* Pose a few questions\n* Clean the data to get ready for analysis\n* Analyze the data to answer the questions\n* Present graphics where needed","metadata":{}},{"cell_type":"code","source":"# import relevant packages\nimport os\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as plticker\nimport seaborn as sns #unused\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n#set up data path\n#this is currently the path to access the data on kaggle, modify as needed\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-14T19:47:08.959124Z","iopub.execute_input":"2021-12-14T19:47:08.959626Z","iopub.status.idle":"2021-12-14T19:47:10.207785Z","shell.execute_reply.started":"2021-12-14T19:47:08.959536Z","shell.execute_reply":"2021-12-14T19:47:10.206821Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#read in data\nlist_df=pd.read_csv('/kaggle/input/seattle/listings.csv')\nreview_df=pd.read_csv('/kaggle/input/seattle/reviews.csv')\ndate_df=pd.read_csv('/kaggle/input/seattle/calendar.csv')\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-12-14T19:47:10.210930Z","iopub.execute_input":"2021-12-14T19:47:10.211472Z","iopub.status.idle":"2021-12-14T19:47:12.093234Z","shell.execute_reply.started":"2021-12-14T19:47:10.211224Z","shell.execute_reply":"2021-12-14T19:47:12.092291Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Explore Dataframes\n\nFirst check the listings dataframe","metadata":{}},{"cell_type":"code","source":"print(list_df.shape)\nlist_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:12.094784Z","iopub.execute_input":"2021-12-14T19:47:12.095218Z","iopub.status.idle":"2021-12-14T19:47:12.136997Z","shell.execute_reply.started":"2021-12-14T19:47:12.095151Z","shell.execute_reply":"2021-12-14T19:47:12.136138Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The listing dataset has 92 columns that describe 3818 property listings","metadata":{}},{"cell_type":"code","source":"#what variables can we play with\nlist_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:12.138124Z","iopub.execute_input":"2021-12-14T19:47:12.138406Z","iopub.status.idle":"2021-12-14T19:47:12.145766Z","shell.execute_reply.started":"2021-12-14T19:47:12.138364Z","shell.execute_reply":"2021-12-14T19:47:12.144576Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#quick summary of data\nlist_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:12.149549Z","iopub.execute_input":"2021-12-14T19:47:12.149878Z","iopub.status.idle":"2021-12-14T19:47:12.250824Z","shell.execute_reply.started":"2021-12-14T19:47:12.149832Z","shell.execute_reply":"2021-12-14T19:47:12.249772Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#get an understanding of categorical and numerical data columns\nlist_df.select_dtypes('number').columns","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:12.254407Z","iopub.execute_input":"2021-12-14T19:47:12.254720Z","iopub.status.idle":"2021-12-14T19:47:12.262661Z","shell.execute_reply.started":"2021-12-14T19:47:12.254675Z","shell.execute_reply":"2021-12-14T19:47:12.261810Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"list_df.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:12.263566Z","iopub.execute_input":"2021-12-14T19:47:12.263830Z","iopub.status.idle":"2021-12-14T19:47:12.489224Z","shell.execute_reply.started":"2021-12-14T19:47:12.263782Z","shell.execute_reply":"2021-12-14T19:47:12.488132Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Next, check the dataframe containing reviews","metadata":{}},{"cell_type":"code","source":"print(review_df.shape)\nreview_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:12.490776Z","iopub.execute_input":"2021-12-14T19:47:12.491154Z","iopub.status.idle":"2021-12-14T19:47:12.504255Z","shell.execute_reply.started":"2021-12-14T19:47:12.491097Z","shell.execute_reply":"2021-12-14T19:47:12.503472Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"almost 85000 comments on the listings in the previous dataset","metadata":{}},{"cell_type":"code","source":"review_df.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:12.505768Z","iopub.execute_input":"2021-12-14T19:47:12.506087Z","iopub.status.idle":"2021-12-14T19:47:12.693407Z","shell.execute_reply.started":"2021-12-14T19:47:12.506036Z","shell.execute_reply":"2021-12-14T19:47:12.692264Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Third, review the calendar dataframe","metadata":{}},{"cell_type":"code","source":"print(date_df.shape)\ndate_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:12.695035Z","iopub.execute_input":"2021-12-14T19:47:12.695486Z","iopub.status.idle":"2021-12-14T19:47:12.708428Z","shell.execute_reply.started":"2021-12-14T19:47:12.695413Z","shell.execute_reply":"2021-12-14T19:47:12.707271Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"date_df.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:12.710548Z","iopub.execute_input":"2021-12-14T19:47:12.711118Z","iopub.status.idle":"2021-12-14T19:47:13.270418Z","shell.execute_reply.started":"2021-12-14T19:47:12.711042Z","shell.execute_reply":"2021-12-14T19:47:13.269406Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print('first date in the dataset is '+date_df['date'].min()+' and last date is '+date_df['date'].max())","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:13.272039Z","iopub.execute_input":"2021-12-14T19:47:13.272552Z","iopub.status.idle":"2021-12-14T19:47:13.476858Z","shell.execute_reply.started":"2021-12-14T19:47:13.272493Z","shell.execute_reply":"2021-12-14T19:47:13.475935Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The calendar dataset contains when the properties are available and their pricing. Some prices are missing. They appear to be if the unit is unavailable Available is boolean but uses t,f instead. It looks like all of the data ares from one year, roughly the year of 2016\n\n## What Problems am I interested in?\nBased on my EDA, there are 3 questions that would be useful or interesting to me for my next trip to Seattle\n1. What is the cheapest time to lodge in Seattle?\n2. Where is the cheapest area to stay?\n3. What are the features that most impact rental price?\n\nBefore answering the questions, the datasets have to be cleaned-up first\n\n## Data Cleaning\n\nFirst clean the listing dataframe","metadata":{}},{"cell_type":"code","source":"#which columns are not missing data\nlen(list_df.columns[list_df.isnull().sum()==0])","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:13.477898Z","iopub.execute_input":"2021-12-14T19:47:13.478180Z","iopub.status.idle":"2021-12-14T19:47:13.499166Z","shell.execute_reply.started":"2021-12-14T19:47:13.478139Z","shell.execute_reply":"2021-12-14T19:47:13.498379Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#how much data is missing per column\nlist_df[list_df.columns[list_df.isnull().sum()>0]].isnull().mean().sort_values()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:13.500278Z","iopub.execute_input":"2021-12-14T19:47:13.500579Z","iopub.status.idle":"2021-12-14T19:47:13.535104Z","shell.execute_reply.started":"2021-12-14T19:47:13.500529Z","shell.execute_reply":"2021-12-14T19:47:13.534373Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Around 10 columns should be dropped due to too much missing data\n\nAlso, there is no missing data for price, our target variable\n\n\nFor finding the cheapest areas, I will need to group the data by neighbourhoods. Let's check if there is enough data for each neighbourhood","metadata":{}},{"cell_type":"code","source":"list_df['neighbourhood_cleansed'].value_counts().hist(bins=40)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:13.536169Z","iopub.execute_input":"2021-12-14T19:47:13.536627Z","iopub.status.idle":"2021-12-14T19:47:13.778163Z","shell.execute_reply.started":"2021-12-14T19:47:13.536587Z","shell.execute_reply":"2021-12-14T19:47:13.777209Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"some neighbourhoods don't have enough data, maybe that's why they also provided a \"neighbourhood groups\" column","metadata":{}},{"cell_type":"code","source":"list_df['neighbourhood_group_cleansed'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:13.779590Z","iopub.execute_input":"2021-12-14T19:47:13.779891Z","iopub.status.idle":"2021-12-14T19:47:13.788195Z","shell.execute_reply.started":"2021-12-14T19:47:13.779842Z","shell.execute_reply":"2021-12-14T19:47:13.787315Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"It is better to analyze the neighbourhood groups, since there is much more data per group","metadata":{}},{"cell_type":"code","source":"#some columns have strange formatting and hey need to be fixed\ndef pricing_reformat(df_col):\n    return df_col.str.strip('$').str.replace(',','').astype(float)\n\nlist_df['price']=pricing_reformat(list_df['price'])\nlist_df['extra_people']=pricing_reformat(list_df['extra_people'])","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:13.789898Z","iopub.execute_input":"2021-12-14T19:47:13.790217Z","iopub.status.idle":"2021-12-14T19:47:13.812650Z","shell.execute_reply.started":"2021-12-14T19:47:13.790166Z","shell.execute_reply":"2021-12-14T19:47:13.811666Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Check missing data for the reviews dataframe. I won't be using this dataframe going forward, so no cleaning is done","metadata":{}},{"cell_type":"code","source":"review_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:13.815879Z","iopub.execute_input":"2021-12-14T19:47:13.816425Z","iopub.status.idle":"2021-12-14T19:47:13.842612Z","shell.execute_reply.started":"2021-12-14T19:47:13.816363Z","shell.execute_reply":"2021-12-14T19:47:13.841305Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Now, clean the calendar data","metadata":{}},{"cell_type":"code","source":"# rows missing prices are not useful since I'm interested in pricing, so drop them\ndate_df2=date_df.dropna(subset=['price'])\ndate_df2.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:13.844073Z","iopub.execute_input":"2021-12-14T19:47:13.844426Z","iopub.status.idle":"2021-12-14T19:47:14.361864Z","shell.execute_reply.started":"2021-12-14T19:47:13.844372Z","shell.execute_reply":"2021-12-14T19:47:14.360578Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# as expected, all rows with prices are for when the listing is available, so the avalability column isn't very useful\ndate_df2=date_df2.drop(['available'],axis=1)\ndate_df2.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:14.363483Z","iopub.execute_input":"2021-12-14T19:47:14.363874Z","iopub.status.idle":"2021-12-14T19:47:14.396723Z","shell.execute_reply.started":"2021-12-14T19:47:14.363816Z","shell.execute_reply":"2021-12-14T19:47:14.395553Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# I want to analyze pricing by month and by date, create relevant columns\ny_md=date_df2['date'].str.split('-',expand=True,n=1)\ndate_df2=date_df2.assign(monthday=y_md[1], month=y_md[1].str.split('-',expand=True)[0].astype(int))\ndate_df2['price']=pricing_reformat(date_df2['price'])\n\ndate_df2.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:14.398011Z","iopub.execute_input":"2021-12-14T19:47:14.398342Z","iopub.status.idle":"2021-12-14T19:47:20.150582Z","shell.execute_reply.started":"2021-12-14T19:47:14.398274Z","shell.execute_reply":"2021-12-14T19:47:20.149316Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#need to check there are enough data for each month and each date\ndate_df2['monthday'].hist(bins=365,xrot=90)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:20.152299Z","iopub.execute_input":"2021-12-14T19:47:20.152747Z","iopub.status.idle":"2021-12-14T19:47:24.057761Z","shell.execute_reply.started":"2021-12-14T19:47:20.152673Z","shell.execute_reply":"2021-12-14T19:47:24.056764Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"OK there should be enough data points per date to make reasonable conclusions\n\nFor modeling to answer my 3rd question, it is best to increase the number of data points. To do this, I can merge list_df and model_df based on their listing IDs","metadata":{}},{"cell_type":"code","source":"list_df=list_df.rename(index=str, columns={'id':'listing_id'})\nmodel_df = pd.merge(date_df2, list_df, on = 'listing_id')\nmodel_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:24.059036Z","iopub.execute_input":"2021-12-14T19:47:24.059345Z","iopub.status.idle":"2021-12-14T19:47:33.654760Z","shell.execute_reply.started":"2021-12-14T19:47:24.059299Z","shell.execute_reply":"2021-12-14T19:47:33.653414Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"We get significantly more data to work with","metadata":{}},{"cell_type":"code","source":"### drop columns\n\n#these cols have way too many missing numbers to be predictive\ndrop_cols=['cleaning_fee','neighborhood_overview','notes','weekly_price','security_deposit','monthly_price','square_feet','license']\nmodel_df=model_df.drop(drop_cols, axis=1)\n\n#to reduce dimensionality of the problem, I decided to drop the following columns\ndrop_cols = ['listing_id', 'monthday','date', 'listing_url', 'scrape_id', 'last_scraped', 'name', 'summary',\n       'space', 'description', 'experiences_offered', 'thumbnail_url', 'medium_url', 'picture_url',\n       'xl_picture_url', 'host_id', 'host_url', 'host_name','host_since', 'host_response_rate',\n       'host_location', 'host_about', 'host_response_time','city','state',\n        'host_acceptance_rate', 'host_is_superhost',\n       'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood',\n       'host_listings_count', 'host_total_listings_count',\n       'host_verifications', 'host_has_profile_pic', 'host_identity_verified',\n       'street', 'neighbourhood', 'neighbourhood_cleansed',\n        'zipcode', 'market', 'transit',\n       'smart_location', 'country_code', 'country', 'latitude', 'longitude',\n       'is_location_exact',  'room_type', 'bed_type', 'amenities', 'minimum_nights',\n       'maximum_nights', 'calendar_updated', 'has_availability',\n       'availability_30', 'availability_60', 'availability_90',\n       'availability_365', 'calendar_last_scraped',\n       'first_review', 'last_review',  'requires_license',\n        'jurisdiction_names', 'instant_bookable',\n       'cancellation_policy', 'require_guest_profile_picture',\n       'require_guest_phone_verification', 'calculated_host_listings_count',\n       'reviews_per_month','price_y']\nmodel_df=model_df.drop(drop_cols, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:33.656271Z","iopub.execute_input":"2021-12-14T19:47:33.656702Z","iopub.status.idle":"2021-12-14T19:47:36.498114Z","shell.execute_reply.started":"2021-12-14T19:47:33.656637Z","shell.execute_reply":"2021-12-14T19:47:36.497176Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#drop any row with missing price\nmodel_df.dropna(subset=['price_x'])\n\n#split response column\ny=model_df['price_x']\nmodel_df=model_df.drop(['price_x'],axis=1)\n\n#fill missing vals for numerical variables\nnum_cols=model_df.select_dtypes(include=['float','int']).columns\nfor col in num_cols:\n    model_df[col].fillna(model_df[col].median(), inplace=True)\n\n#dummy variables for categorical variables\ncat_cols=model_df.select_dtypes(include=['object']).columns\nfor col in cat_cols:\n    model_df=pd.concat([model_df.drop(col, axis=1), pd.get_dummies(model_df[col], prefix=col, prefix_sep='_', drop_first=True)], axis=1)\n\nmodel_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:36.499529Z","iopub.execute_input":"2021-12-14T19:47:36.499851Z","iopub.status.idle":"2021-12-14T19:47:37.270593Z","shell.execute_reply.started":"2021-12-14T19:47:36.499799Z","shell.execute_reply":"2021-12-14T19:47:37.269752Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Analysis\n## Q1: Cheapest Time to visit Seattle\n\nApproach:\n* group by month and calculate average price\n* group by date and calculate average price\n* plot the results\n\nThe results will show the cheapest month and dates","metadata":{}},{"cell_type":"code","source":"#calculate monthly and daily price averages\nmonthly_prices=date_df2.groupby(['month'], as_index=False, group_keys=False)['price']\ndaily_prices=date_df2.groupby(['monthday'], as_index=False, group_keys=False)['price']\nmonthly_avg=monthly_prices.mean()\ndaily_avg=daily_prices.mean()\ndaily_avg","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:37.272317Z","iopub.execute_input":"2021-12-14T19:47:37.272627Z","iopub.status.idle":"2021-12-14T19:47:37.445422Z","shell.execute_reply.started":"2021-12-14T19:47:37.272576Z","shell.execute_reply":"2021-12-14T19:47:37.444257Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nplt.plot(monthly_avg['month'],monthly_avg['price'])\nplt.xlabel('month')\nplt.ylabel('prices per day')\nplt.title('average price by month')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:37.446697Z","iopub.execute_input":"2021-12-14T19:47:37.446978Z","iopub.status.idle":"2021-12-14T19:47:37.605191Z","shell.execute_reply.started":"2021-12-14T19:47:37.446943Z","shell.execute_reply":"2021-12-14T19:47:37.603991Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(f'the cheapest month is January with average price of ${monthly_avg.price.min():.2f}')\nprint(f'the most expensive month is July with average price of ${monthly_avg.price.max():.2f}')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:37.606967Z","iopub.execute_input":"2021-12-14T19:47:37.607428Z","iopub.status.idle":"2021-12-14T19:47:37.613903Z","shell.execute_reply.started":"2021-12-14T19:47:37.607353Z","shell.execute_reply":"2021-12-14T19:47:37.612754Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\nfig, ax = plt.subplots()\nax.plot(daily_avg['monthday'],daily_avg['price'])\nplt.xlabel('date')\nplt.xticks(rotation=90, fontsize=8)\nloc = plticker.MultipleLocator(base=14.0) \nax.xaxis.set_major_locator(loc)\nplt.ylabel('prices per day')\nplt.title('average price by date')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:37.615493Z","iopub.execute_input":"2021-12-14T19:47:37.616019Z","iopub.status.idle":"2021-12-14T19:47:38.139433Z","shell.execute_reply.started":"2021-12-14T19:47:37.615851Z","shell.execute_reply":"2021-12-14T19:47:38.138487Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"max_val=daily_avg['price'].max()\nmax_date=daily_avg.iloc[daily_avg['price'].argmax()].monthday\nmin_val=daily_avg['price'].min()\nmin_date=daily_avg.iloc[daily_avg['price'].argmin()].monthday\nprint(f'maximum daily average prices is ${max_val:.2f} on {max_date}')\nprint(f'minimum daily average prices is ${min_val:.2f} on {min_date}')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:38.140600Z","iopub.execute_input":"2021-12-14T19:47:38.140892Z","iopub.status.idle":"2021-12-14T19:47:38.150428Z","shell.execute_reply.started":"2021-12-14T19:47:38.140842Z","shell.execute_reply":"2021-12-14T19:47:38.149429Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"There are spikes in the prices is due to the weekends. I want to get a better sense of how prices change week to week. To do so, I apply a rolling average of 7 days to smooth things over.","metadata":{}},{"cell_type":"code","source":"daily_avg_smooth=daily_avg.rolling(7, center=True).mean()  \ndaily_avg_smooth.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:38.152165Z","iopub.execute_input":"2021-12-14T19:47:38.152494Z","iopub.status.idle":"2021-12-14T19:47:38.171061Z","shell.execute_reply.started":"2021-12-14T19:47:38.152453Z","shell.execute_reply":"2021-12-14T19:47:38.170118Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"We are loosing a bit of data at the beginning and end of the year, but they shouldn't affect our results too much","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(daily_avg['monthday'],daily_avg_smooth['price'])\nplt.xlabel('date')\nplt.xticks(rotation=90, fontsize=8)\nloc = plticker.MultipleLocator(base=14.0) \nax.xaxis.set_major_locator(loc)\nplt.ylabel('prices per day')\nplt.title('average price by date')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:38.175049Z","iopub.execute_input":"2021-12-14T19:47:38.175654Z","iopub.status.idle":"2021-12-14T19:47:38.676631Z","shell.execute_reply.started":"2021-12-14T19:47:38.175339Z","shell.execute_reply":"2021-12-14T19:47:38.675517Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"max_val=daily_avg_smooth['price'].max()\nmax_date=daily_avg.iloc[daily_avg_smooth['price'].argmax()].monthday\nmin_val=daily_avg_smooth['price'].min()\nmin_date=daily_avg.iloc[daily_avg_smooth['price'].argmin()].monthday\nprint(f'maximum daily average prices is ${max_val:.2f} in the week +/-3 days around {max_date}')\nprint(f'minimum daily average prices is ${min_val:.2f} in the week +/-3 days around {min_date}')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:38.678954Z","iopub.execute_input":"2021-12-14T19:47:38.679749Z","iopub.status.idle":"2021-12-14T19:47:38.688954Z","shell.execute_reply.started":"2021-12-14T19:47:38.679519Z","shell.execute_reply":"2021-12-14T19:47:38.687715Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Peak prices are actually centered around July 4th (Independence Day in Seattle), especially since in 2016 July 4th was on a Monday, making it a long weekend that many people would want to travel. On the other hand, late January is when prices fall to its minimum\n\n## Question 2: cheapest area\n\nApproach\n* Group listings by neighbourhoods and find average price for each area\n\nThe results will show which areas are the cheapest to stay","metadata":{}},{"cell_type":"code","source":"area_prices=list_df.groupby(['neighbourhood_group_cleansed'], as_index=False, group_keys=False)['price']\narea_avg=area_prices.mean().sort_values(['price'])\narea_avg","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:38.690075Z","iopub.execute_input":"2021-12-14T19:47:38.690542Z","iopub.status.idle":"2021-12-14T19:47:38.711083Z","shell.execute_reply.started":"2021-12-14T19:47:38.690484Z","shell.execute_reply":"2021-12-14T19:47:38.709778Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"plt.bar(area_avg['neighbourhood_group_cleansed'],area_avg['price'])\nplt.xticks(rotation=90)\nplt.xlabel('Neighbourhood')\nplt.ylabel('prices per day')\nplt.title('Average price by area')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:47:38.712573Z","iopub.execute_input":"2021-12-14T19:47:38.713040Z","iopub.status.idle":"2021-12-14T19:47:38.926832Z","shell.execute_reply.started":"2021-12-14T19:47:38.712846Z","shell.execute_reply":"2021-12-14T19:47:38.925744Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"This makes it clear the cheapest places to lodge is Delridge, followed closely by Northgate, Rainier Valley, Lake City, University District, and Beacon Hill\n\nThose most expensive place, and by a significant margin, is Magnolia.\n\n## Q3: Top factors influencing price\n\nNow I want to figure out what are the top factors that affect price.\n\nApproach:\n* create a model that can accurately predict prices\n* examine the weights for each factor\n\nI used the random forest regression model and the r2 and MSE criteria to evaluation\n","metadata":{}},{"cell_type":"code","source":"\n## modelling\nX_train, X_test, y_train, y_test = train_test_split(model_df, y, test_size = 0.2, random_state=7)\n#lin_model = LinearRegression(normalize=True) # Instantiate\nrf_model = RandomForestRegressor(n_estimators=150, \n                               criterion='mse', random_state=7, n_jobs=-1)\n\nrf_model.fit(X_train, y_train) #Fit\n\nresult_train = rf_model.predict(X_train)\nresult_test = rf_model.predict(X_test)\n    \ntest_score = r2_score(y_test, result_test)\ntrain_score= r2_score(y_train, result_train)\n\nprint('R2 test score: '+str(test_score))\nprint('R2 train score: '+str(train_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nX_train, X_test, y_train, y_test = train_test_split(model_df, y, test_size = 0.2, random_state=7)\n\nrf_model = RandomForestRegressor(random_state=7, n_jobs=-1, oob_score=True, verbose=1)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T21:34:34.237067Z","iopub.execute_input":"2021-12-14T21:34:34.237442Z","iopub.status.idle":"2021-12-14T21:34:34.621971Z","shell.execute_reply.started":"2021-12-14T21:34:34.237395Z","shell.execute_reply":"2021-12-14T21:34:34.621092Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"param_grid = {'max_depth': randint(5,30), \n              'max_features': randint(5,30),\n              'n_estimators':randint(100,500),\n              'min_samples_split':randint(2,5)}\n\nrf_cv =RandomizedSearchCV(rf_model, param_grid, scoring=\"r2\",     # (and our parameter grid) to a new instance\n                      n_jobs=-1, verbose=1, n_iter=8, cv=3)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T22:29:47.892007Z","iopub.execute_input":"2021-12-14T22:29:47.892655Z","iopub.status.idle":"2021-12-14T22:29:47.903774Z","shell.execute_reply.started":"2021-12-14T22:29:47.892578Z","shell.execute_reply":"2021-12-14T22:29:47.902879Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"rf_cv.fit(X_train, y_train)\nprint(rf_cv.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T22:30:03.283953Z","iopub.execute_input":"2021-12-14T22:30:03.285316Z","iopub.status.idle":"2021-12-14T23:44:51.171673Z","shell.execute_reply.started":"2021-12-14T22:30:03.285219Z","shell.execute_reply":"2021-12-14T23:44:51.169097Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"print(rf_cv.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:47:54.827649Z","iopub.execute_input":"2021-12-14T23:47:54.828014Z","iopub.status.idle":"2021-12-14T23:47:54.833951Z","shell.execute_reply.started":"2021-12-14T23:47:54.827963Z","shell.execute_reply":"2021-12-14T23:47:54.832211Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"rf_final = RandomForestRegressor(random_state=7, n_jobs=-1, oob_score=True, max_depth =27, max_features=20, min_samples_split=3, n_estimators=308)\n\nrf_final.fit(X_train, y_train)\n\nresult_train = rf_final.predict(X_train)\nresult_test = rf_final.predict(X_test)\n\ntest_score = r2_score(y_test, result_test)\ntrain_score= r2_score(y_train, result_train)\n\nprint('R2 test score: '+str(test_score))\nprint('R2 train score: '+str(train_score))\n\n\ntest_score = mean_squared_error(y_test, result_test)\ntrain_score= mean_squared_error(y_train, result_train)\n\nprint('MSE test score: '+str(test_score))\nprint('MSE train score: '+str(train_score))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:11:36.522340Z","iopub.execute_input":"2021-12-15T00:11:36.522772Z","iopub.status.idle":"2021-12-15T00:12:06.558176Z","shell.execute_reply.started":"2021-12-15T00:11:36.522721Z","shell.execute_reply":"2021-12-15T00:12:06.557026Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"headers = [\"name\", \"score\"]\nvalues = sorted(zip(X_train.columns, rf_final.feature_importances_), key=lambda x: x[1] * -1)\nforest_feature_importances = pd.DataFrame(values, columns = headers)\nforest_feature_importances = forest_feature_importances.sort_values(by = ['score'], ascending = False)\n\nfeatures = forest_feature_importances['name'][:20]\ny_pos = np.arange(len(features))\nscores = forest_feature_importances['score'][:20]\n\n#plot feature importances\nplt.figure()\nplt.bar(y_pos, scores, align='center', alpha=0.5)\nplt.xticks(y_pos, features, rotation='vertical')\nplt.ylabel('Score')\nplt.xlabel('Features')\nplt.title('Feature importances')\n \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:13:57.890381Z","iopub.execute_input":"2021-12-15T00:13:57.891132Z","iopub.status.idle":"2021-12-15T00:13:58.256371Z","shell.execute_reply.started":"2021-12-15T00:13:57.891077Z","shell.execute_reply":"2021-12-15T00:13:58.255252Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"Not too surprisingly, the most important determinants of price are the size and location of the property. Time of the year and the number of reviews also influences price.\n\nSince number of reviews is a top factor affecting price, one follow-up question is: am I more likely to get a worse rental if I go for one with less reviews?\n\nA data fit should help us to see if there is any correlation","metadata":{}},{"cell_type":"code","source":"list_df2 = list_df[list_df['number_of_reviews']>5] #get rid of extremes that might skew data \nplt.figure()\nplt.plot(list_df2['number_of_reviews'],list_df2['review_scores_rating'],'o',label='data')\nlin_fit_param=np.polyfit(list_df2['number_of_reviews'], list_df2['review_scores_rating'],1)\nplt.plot(list_df2['number_of_reviews'], lin_fit_param[0]*list_df2['number_of_reviews']+lin_fit_param[1],0, label='fit line')\nplt.xlabel('number of reviews')\nplt.ylabel('average score of reviews')\nplt.title('Does more reviews mean better reviews?')\nplt.legend(loc='lower right')\nprint(f'slope of linear fit is {lin_fit_param[0]:.5f}')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:14:04.560740Z","iopub.execute_input":"2021-12-15T00:14:04.561106Z","iopub.status.idle":"2021-12-15T00:14:04.840157Z","shell.execute_reply.started":"2021-12-15T00:14:04.561056Z","shell.execute_reply":"2021-12-15T00:14:04.839313Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"There is a very weak correlation.\n\nIf we group the data by how many reviews they get, we can also see that the difference is very small. I think it is safe to say it is fine to go for rentals with less reviews.","metadata":{}},{"cell_type":"code","source":"list_df3 = list_df[list_df['number_of_reviews']>=20]  #number of rentals with more than 20 reviews\nlist_df4 = list_df[list_df['number_of_reviews']<20] \nprint(list_df3.shape) #check that the number of data points is roughly the same\nprint(list_df4.shape)\nx=['less than 20','more than or equal 20']\ny=[list_df3['review_scores_rating'].mean(),list_df4['review_scores_rating'].mean()]\nplt.figure()\nplt.bar(x,y)\nplt.xlabel('number of reviews')\nplt.ylabel('average score')\nplt.title('Average score compared with number of reviews')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:14:07.455102Z","iopub.execute_input":"2021-12-15T00:14:07.455784Z","iopub.status.idle":"2021-12-15T00:14:07.599450Z","shell.execute_reply.started":"2021-12-15T00:14:07.455731Z","shell.execute_reply":"2021-12-15T00:14:07.598304Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}